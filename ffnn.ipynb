{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(n_in, n_out, n_hiddens = []):\n",
    "    \"\"\"\n",
    "    Builds a feed forward neural network with `n_in` input\n",
    "    neurons connected through `len(n_hiddens)` hidden layers,\n",
    "    each with the amount of neurons specified by the elements\n",
    "    of `n_hiddens` and leading to an output layer of `n_out` \n",
    "    neurons. Returns a list of (weights, biases) tuples which\n",
    "    can later be fed into the `feed_forward` function.\n",
    "    \"\"\"\n",
    "    network = []\n",
    "    sizes = [n_in] + n_hiddens + [n_out]\n",
    "    \n",
    "    # Loop over all layers in pairs of two adjacent at a time.\n",
    "    for left, right in zip(sizes[:-1], sizes[1:]):\n",
    "        weights = tf.Variable(tf.truncated_normal([left, right]))\n",
    "        biases = tf.Variable(tf.zeros([right]))\n",
    "        network.append((weights, biases))\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(layers, data):\n",
    "    \"\"\"\n",
    "    Applies the feed forward neural network given in\n",
    "    layers to the data.\n",
    "    \"\"\"\n",
    "    for (weights, biases) in layers[:-1]: # All but last.\n",
    "        data = tf.sigmoid(tf.matmul(data, weights) + biases)\n",
    "    \n",
    "    # Output neurons do not use an activation function.\n",
    "    weights_out, biases_out = layers[-1]\n",
    "    return tf.matmul(data, weights_out) + biases_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions given in y\n",
    "    in relation to the actual target labels given in y_.\n",
    "    \"\"\"\n",
    "    correct_predictions = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    return tf.reduce_mean(tf.cast(correct_predictions, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def ffnn(train, test = None, epochs=10000, rate=0.01, hidden=[]):\n",
    "    data, targets = train\n",
    "    \n",
    "    samples, n_in  = data.shape\n",
    "    _,       n_out = targets.shape\n",
    "\n",
    "    # build up TensorFlow data flow graph by initializing variables and\n",
    "    # placeholders and composing functions.\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_in])  # Input\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_out]) # Target Output\n",
    "    \n",
    "    model = neural_net(n_in, n_out, n_hiddens=hidden) # Network.\n",
    "    pred  = feed_forward(model, x) # Network Output.\n",
    "    \n",
    "    # Cost function and cost optimizer/training step.\n",
    "    cost_fn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(rate).minimize(cost_fn)\n",
    "    \n",
    "    # If no test data is provided we test the network using the training data.\n",
    "    test_data, test_targets = test if test is not None else train\n",
    "    test_pred = tf.nn.softmax(feed_forward(model, test_data))\n",
    "    test_accu = accuracy(test_pred, y)\n",
    "\n",
    "    # Range used for sampling inside the training loop.\n",
    "    take_all = take = [i for i in range(n_in)]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training loop.\n",
    "        for epoch in range(1, epochs+1):\n",
    "            # If there is a lot of test data take a new \n",
    "            # random sample in each iteration.\n",
    "            if n_in > 500:\n",
    "                shuffle(take_all)\n",
    "                take = take_all[0:500]\n",
    "            \n",
    "            # Run the optimizer with a sample of the data.\n",
    "            sess.run(optimizer, feed_dict={ x: data[take], y: targets[take] })\n",
    "            \n",
    "            # Print the progress and accuracy when using the test data.\n",
    "            if (epoch % 200) == 0:\n",
    "                performance = sess.run(test_accu, feed_dict={y: test_targets})\n",
    "                print('Epoch %d with accuracy %.2f%%.' % (epoch, performance*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "data, targets = mnist.train.next_batch(55000)\n",
    "ffnn(train=(data, targets), test=(mnist.test.images, mnist.test.labels), hidden=[200], epochs=20000)\n",
    "\n",
    "## MNIST Results\n",
    "# hidden=[200], epochs=20000, GradiendDescentOptimizer, softmax_cross_entropy_with_logits --> 71.83%\n",
    "# hidden=[128, 32], epochs=20000, GradiendDescentOptimizer, softmax_cross_entropy_with_logits --> 73.29%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
